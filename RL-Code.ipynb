{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -N https://rl-learning-storage.s3.amazonaws.com/asteroids.zip\n",
    "!unzip -o asteroids.zip\n",
    "!pip install gym-retro\n",
    "#!pip3 install pyglet==1.5.0 gym-retro\n",
    "!python3 -m retro.import ./\n",
    "!pip install keras-rl\n",
    "!pip install tensorflow==1.14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install wrapt --upgrade --ignore-installed\n",
    "pip install tensorflow==1.14\n",
    "pip install keras-rl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "path = '/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/retro/data/stable/Asteroids-Atari2600'\n",
    "\n",
    "directory_contents = os.listdir(path)\n",
    "print(directory_contents)\n",
    "\n",
    "for item in directory_contents:\n",
    "    if os.path.isdir(item):\n",
    "        print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.remove('/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/retro/data/stable/Asteroids-Atari2600/scenario.json')\n",
    "shutil.move('/home/ec2-user/SageMaker/scenario.json', '/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/retro/data/stable/Asteroids-Atari2600/scenario.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import retro\n",
    "#Create Environment for Asteroid\n",
    "ENV_NAME = 'Asteroids-Atari2600'\n",
    "env = retro.make(game=ENV_NAME, record='.', use_restricted_actions=retro.Actions.DISCRETE)\n",
    "nb_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BUTTON', None, 'SELECT', 'RESET', 'UP', 'DOWN', 'LEFT', 'RIGHT']\n",
      "Discrete(18)\n",
      "Box(210, 160, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "210"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check Game Properties\n",
    "print(env.buttons)\n",
    "print(env.action_space)\n",
    "print(env.observation_space)\n",
    "state_shape = env.observation_space.shape\n",
    "state_shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 100800)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 420)               42336420  \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 420)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 18)                7578      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 18)                0         \n",
      "=================================================================\n",
      "Total params: 42,343,998\n",
      "Trainable params: 42,343,998\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten,Convolution3D, MaxPooling2D, InputLayer\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.agents import SARSAAgent,DDPGAgent\n",
    "from rl.policy import EpsGreedyQPolicy,LinearAnnealedPolicy,BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.callbacks import FileLogger, ModelIntervalCheckpoint\n",
    "#Determine the structure of neural net\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "model.add(Dense(420))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_actions))\n",
    "model.add(Activation('linear'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/anaconda3/lib/python3.7/site-packages/rl/util.py:79: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Training for 50000 steps ...\n",
      "WARNING:tensorflow:From /opt/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "  2183/50000: episode: 1, duration: 328.135s, episode steps: 2183, steps per second: 7, episode reward: -63999006.000, mean reward: -29316.998 [-15999984.000, 100.000], mean action: 6.535 [0.000, 17.000], mean observation: 1.824 [0.000, 240.000], loss: 22057.419435, mae: 1235.783317, mean_q: 29.434680, mean_eps: 0.100000\n",
      "  3822/50000: episode: 2, duration: 481.445s, episode steps: 1639, steps per second: 3, episode reward: -63999356.000, mean reward: -39047.807 [-15999984.000, 100.000], mean action: 9.126 [0.000, 17.000], mean observation: 2.266 [0.000, 240.000], loss: 45126.480889, mae: 2511.957607, mean_q: 10.463821, mean_eps: 0.100000\n",
      "  5427/50000: episode: 3, duration: 522.614s, episode steps: 1605, steps per second: 3, episode reward: -63999506.000, mean reward: -39875.082 [-15999984.000, 100.000], mean action: 9.897 [0.000, 17.000], mean observation: 2.088 [0.000, 240.000], loss: 28680.566893, mae: 1597.524202, mean_q: 7.372575, mean_eps: 0.100000\n",
      "  7082/50000: episode: 4, duration: 442.893s, episode steps: 1655, steps per second: 4, episode reward: -63999356.000, mean reward: -38670.306 [-15999984.000, 100.000], mean action: 6.724 [0.000, 17.000], mean observation: 2.804 [0.000, 240.000], loss: 39856.838301, mae: 2218.283089, mean_q: 5.980913, mean_eps: 0.100000\n",
      " 11395/50000: episode: 5, duration: 991.918s, episode steps: 4313, steps per second: 4, episode reward: -63998956.000, mean reward: -14838.617 [-15999984.000, 100.000], mean action: 8.300 [0.000, 17.000], mean observation: 1.596 [0.000, 240.000], loss: 34324.471580, mae: 1910.364632, mean_q: 4.719997, mean_eps: 0.100000\n",
      " 14178/50000: episode: 6, duration: 657.702s, episode steps: 2783, steps per second: 4, episode reward: -63999006.000, mean reward: -22996.409 [-15999984.000, 100.000], mean action: 8.238 [0.000, 17.000], mean observation: 1.828 [0.000, 240.000], loss: 25863.098498, mae: 1439.998063, mean_q: 4.354812, mean_eps: 0.100000\n",
      " 15835/50000: episode: 7, duration: 402.781s, episode steps: 1657, steps per second: 4, episode reward: -63999626.000, mean reward: -38623.794 [-15999984.000, 100.000], mean action: 7.500 [0.000, 17.000], mean observation: 2.401 [0.000, 240.000], loss: 26570.538958, mae: 1478.828412, mean_q: 3.658465, mean_eps: 0.100000\n",
      " 18022/50000: episode: 8, duration: 586.635s, episode steps: 2187, steps per second: 4, episode reward: -63998956.000, mean reward: -29263.354 [-15999984.000, 100.000], mean action: 8.957 [0.000, 17.000], mean observation: 2.099 [0.000, 240.000], loss: 32907.675975, mae: 1830.621259, mean_q: 3.244976, mean_eps: 0.100000\n",
      " 19677/50000: episode: 9, duration: 535.149s, episode steps: 1655, steps per second: 3, episode reward: -63999506.000, mean reward: -38670.396 [-15999984.000, 100.000], mean action: 7.795 [0.000, 17.000], mean observation: 2.728 [0.000, 240.000], loss: 36231.889075, mae: 2015.083563, mean_q: 2.936850, mean_eps: 0.100000\n",
      " 22738/50000: episode: 10, duration: 966.591s, episode steps: 3061, steps per second: 3, episode reward: -63999106.000, mean reward: -20907.908 [-15999984.000, 100.000], mean action: 8.235 [0.000, 17.000], mean observation: 2.313 [0.000, 240.000], loss: 26142.971626, mae: 1454.567472, mean_q: 2.905544, mean_eps: 0.100000\n",
      " 24957/50000: episode: 11, duration: 719.430s, episode steps: 2219, steps per second: 3, episode reward: -63999506.000, mean reward: -28841.598 [-15999984.000, 50.000], mean action: 8.583 [0.000, 17.000], mean observation: 2.471 [0.000, 240.000], loss: 24323.002209, mae: 1353.551571, mean_q: 2.995583, mean_eps: 0.100000\n",
      " 27362/50000: episode: 12, duration: 795.916s, episode steps: 2405, steps per second: 3, episode reward: -63999256.000, mean reward: -26610.917 [-15999984.000, 100.000], mean action: 8.536 [0.000, 17.000], mean observation: 2.113 [0.000, 240.000], loss: 34110.601311, mae: 1896.610800, mean_q: 2.146853, mean_eps: 0.100000\n",
      " 30501/50000: episode: 13, duration: 969.103s, episode steps: 3139, steps per second: 3, episode reward: -63998956.000, mean reward: -20388.326 [-15999984.000, 100.000], mean action: 8.570 [0.000, 17.000], mean observation: 2.200 [0.000, 240.000], loss: 25478.399462, mae: 1417.062906, mean_q: 2.098672, mean_eps: 0.100000\n"
     ]
    }
   ],
   "source": [
    "#Decide Policy Memory for agent\n",
    "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps', value_max=1., value_min=.1, value_test=.05,nb_steps=500)\n",
    "memory = SequentialMemory(limit=100000000, window_length=1)\n",
    "dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, gamma=0.99, nb_steps_warmup=1000,target_model_update=1000, policy=policy,enable_double_dqn=True,train_interval=4, delta_clip=1.)\n",
    "dqn.compile(Adam(lr=.00025), metrics=['mae'])\n",
    "\n",
    "# Train the model \n",
    "steps = 50000\n",
    "max_reward = 0\n",
    "\n",
    "def build_callbacks(env_name):\n",
    "    checkpoint_weights_filename = 'dqn_' + ENV_NAME + '_weights_{step}.h5f'\n",
    "    log_filename = 'dqn_{}_log.json'.format(env_name)\n",
    "    callbacks = [ModelIntervalCheckpoint(checkpoint_weights_filename, interval=1000)]\n",
    "    callbacks += [FileLogger(log_filename, interval=100)]\n",
    "    return callbacks\n",
    "\n",
    "callbacks = build_callbacks(ENV_NAME)\n",
    "dqn.fit(env, nb_steps=steps, visualize=False,verbose=2,callbacks=callbacks,log_interval=1000)\n",
    "dqn.save_weights('dqn_' + ENV_NAME + '_weights_{step}.h5f', overwrite=True)\n",
    "test = dqn.test(env, nb_episodes = 3, visualize=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import re\n",
    "import copy\n",
    "import time\n",
    "import io\n",
    "import struct\n",
    "from time import gmtime, strftime\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "bucket='rl-learning-storage' # Replace with your s3 bucket name\n",
    "prefix = 'RL-Information' # Used as part of the path in the bucket where you store data\n",
    "url = 's3://{}/{}'.format(bucket, prefix)\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(prefix).upload_file('*.h5f')\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(prefix).upload_file('*.bk2')\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(prefix).upload_file('*.json')\n",
    "print('Done writing to {}'.format(url))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
